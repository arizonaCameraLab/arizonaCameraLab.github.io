---
title: "Camera Lab"
layout: homelay
excerpt: "AISI"
sitemap: false
permalink: /
---

<!-- <div markdown="0" id="carousel" class="carousel slide" data-ride="carousel" data-interval="4000" data-pause="hover" >
<ol class="carousel-indicators">
<li data-target="#carousel" data-slide-to="0" class="active"></li>
<li data-target="#carousel" data-slide-to="1" ></li>
<li data-target="#carousel" data-slide-to="2"></li>
<li data-target="#carousel" data-slide-to="3"></li>
<li data-target="#carousel" data-slide-to="4"></li>
</ol>
<div class="carousel-inner" markdown="0">


<div class="item active">
    <img src="{{ site.url }}{{ site.baseurl }}/assets/images/slider/gigapixel_camera.png" alt="Slide 0" oncontextmenu="return false;" />
</div>
<div class="item ">
    <img src="{{ site.url }}{{ site.baseurl }}/assets/images/slider/gigapixel_camera.png" alt="Slide 1" oncontextmenu="return false;" />
</div>
<div class="item">
    <img src="{{ site.url }}{{ site.baseurl }}/assets/images/slider/gigapixel_camera.png" alt="Slide 2" oncontextmenu="return false;" />
</div>
<div class="item">
    <img src="{{ site.url }}{{ site.baseurl }}/assets/images/slider/gigapixel_camera.png" alt="Slide 3" oncontextmenu="return false;" />
</div>
<div class="item">
    <img src="{{ site.url }}{{ site.baseurl }}/assets/images/slider/gigapixel_camera.png" alt="Slide 4" oncontextmenu="return false;" />
</div>
</div>

</div> -->


<!-- ## Welcome

The Duke Information Spaces Project develops sensor arrays for real-time virtual reality broadcasting. As an example, the image shown above was captured by the DISP's AWARE10 camera at the 2013 ACC Championship Game. Click here to view an AWARE10 image interactively. Click [here](http://www.gigapan.com/gigapans/146504) to see more shots from the game.  Current research topics include 

(1) sensor system design, including lens design and computer architecture, to capture and broadcast events in real-time and 
(2) The use of artificial intelligence to
- control capture parameters, such as focus, exposure and attention in camera arrays
- manage gigapixel/second data streams, e.g. compress and analyze
- and composite virtual reality from sensor data streams. 

DISP was previously known as the Duke Imaging and Spectroscopy Program, which focused on physical layer coding, generalized sampling and nonlinear signal processing to build optical imaging and spectroscopy systems spanning x-ray to radio wave frequencies. DISP is affiliated with the Department of Electrical and Computer Engineering, the Fitzpatrick Institute for Photonics and the Pratt School of Engineering at Duke University.

More details on DISP can be found by exploring the publications on our [google scholar](https://scholar.google.com/citations?user=CcSZwTsAAAAJ&hl=en) page or by looking at the textbook [Optical Imaging and Spectroscopy](http://www.opticalimaging.org/).  -->



## THE CAMERA LAB

The camera lab develops gigapixel media machines. At the invention of photography, recording was so slow that a person had to stand still for several minutes while the exposure developed, as illustrated in Dagaurre’s famous 1838 shot of the [Boulevard du Temple](https://en.wikipedia.org/wiki/Boulevard_du_Temple_(photograph)). 60 years later, folks began to make movies (see Le Prince’s [Roundhay Garden Scene](https://en.wikipedia.org/wiki/Roundhay_Garden_Scene)). Celluoid film was the primary photographic medium for most of the next century, supplemented by vidicon cameras in the 1930’s and charge coupled devices in the 1980’s. With the development of active pixel sensors and image signal processing chips in the 1990’s, photography entered a new age.

The modern camera consists of optics, electronic focal planes and a computer. While the basic idea of a camera with a lens and a focal plane is not fundamentally different than the original Daguerre model, once computation becomes part of the camera the machine changes fundamentally. The recovered image is no longer the physical image formed by the lens, rather it is created by digital analysis. The digital system may use light collected over different times and different apertures to estimate the scene and may use color, polarization and active illumination.

The electronic telegraph, developed in Daguerre’s era, sent information at rates of perhaps 10 characters per second. Modern fiber optical communications improve on this speed by 13 orders of magnitude. Daguerre captures around 300 pixels/second. Modern 4k video at 60 fps captures 480 megapixels/second, a million-fold improvement on Daguerre.

The Camera Lab asks the question: **Why haven’t cameras improved as much as other information technologies?** Our goal is to capture all available optical information. When you visit a place, say [a wildlife sanctuary](https://rdcu.be/cOxHn), do you ever think, "this is nice but I wish I could look at a photograph. Photographs are so much better than actually being here." You don’t, but you should.

The Camera Lab is led by David Brady, who in 2012 led the team [that developed the world’s first gigapixel camera](https://www.youtube.com/watch?v=ldQXHP7TFUE) as principal investigator on the DARPA AWARE program. Click [here](http://www.gigapan.com/gigapans/146504) to see a snapshot taken by an AWARE camera. AWARE cameras were capable of 24 fps video recording, exceeding the information rate of Daguerre’s camera by around 100 million. But we can do better. Over the past decade, Brady and his colleagues have continued to develop wide field of view high resolution high frame rate cameras. Click [here](https://aqueti.vids.io/videos/a09dd8b41c1ee0c728/videowall-mp4), for example, is video from a more recent camera in action.

The Camera Lab develops optical designs, algorithms and systems to continue to press photography toward the physical limits of resolution. We firmly believe that it is possible for the effective information rate of cameras to exceed the Daguerreotype by a trillion times and we are building the systems to do it. Our recent work shows that >10cm camera aperture at >1 megahertz effective frame rate with >100 color channels is possible. But we must be clever about how we balance physics and computation. We use compressive measurement and artificial neural networks to make extreme resolution systems practical and fun to use.



